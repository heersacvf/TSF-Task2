# -*- coding: utf-8 -*-
"""TSF Task 2-Unsupervised Learning Using ML (K means-clustering)& data Visualization by HEERA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HoxL3fM6DM5yKX6ML--9O16MrQLAzVU3
"""

##importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn import datasets

import seaborn as sns
import os

"""Including the **libraries** **bold text** **"""

mydata = pd.read_csv("/Iris.csv")
mydata

mydata.head()

"""Exploring the data-to display stats about data"""

mydata.describe() #mean ,std deviation etc ##to display stats

##to know the data types we have in our data
mydata.info()

print(mydata.head())

"""Creating color Dictionary for easily identifying different species"""

# create color dictionary
colors = {'Iris-setosa':'g', 'Iris-versicolor':'b', 'Iris-virginica':'r'}

"""Data scatter Plots with color"""

colors = {'Iris-setosa':'g', 'Iris-versicolor':'b', 'Iris-virginica':'r'}
fig, cx = plt.subplots()

for i in range(len(mydata['SepalLengthCm'])):
    cx.scatter(mydata['SepalLengthCm'][i], mydata['SepalWidthCm'][i],color=colors[mydata['Species'][i]])

cx.set_title('Iris Dataset visualisation with colors')
cx.set_xlabel('sepal length in cms')
cx.set_ylabel('sepal width in cms')

colors = {'Iris-setosa':'g', 'Iris-versicolor':'b', 'Iris-virginica':'r'}
fig, dx = plt.subplots()

for i in range(len(mydata['PetalLengthCm'])):
    dx.scatter(mydata['PetalLengthCm'][i], mydata['PetalWidthCm'][i],color=colors[mydata['Species'][i]])

dx.set_title('Iris Dataset visualisation with colors')
dx.set_xlabel('Petal length in cms')
dx.set_ylabel('Petal width in cms')

colors = {'Iris-setosa':'g', 'Iris-versicolor':'b', 'Iris-virginica':'r'}
fig, dx = plt.subplots()

for i in range(len(mydata['PetalLengthCm'])):
    dx.scatter(mydata['PetalLengthCm'][i], mydata['SepalLengthCm'][i],color=colors[mydata['Species'][i]])

dx.set_title('Iris Dataset visualisation with colors')
dx.set_xlabel('Petal length in cms')
dx.set_ylabel('Sepal Length in cms')

colors = {'Iris-setosa':'g', 'Iris-versicolor':'b', 'Iris-virginica':'r'}
fig, dx = plt.subplots()

for i in range(len(mydata['PetalWidthCm'])):
    dx.scatter(mydata['PetalWidthCm'][i], mydata['SepalWidthCm'][i],color=colors[mydata['Species'][i]])

dx.set_title('Iris Dataset visualisation with colors')
dx.set_xlabel('Petal Width in cms')
dx.set_ylabel('Sepal Width in cms')

"""Line Chart to visualize Data"""

# get columns to plot
columns = mydata.columns.drop(['Species'])
# create x data
x_data = range(0, mydata.shape[0])
# create figure and axis
fig, ex = plt.subplots()
# plot each column
for column in columns:
    ex.plot(x_data, mydata[column], label=column)
# set title and legend
cx.set_title('Iris Dataset line chart  ')
cx.legend()

"""Histograms"""

fig, fx = plt.subplots()
# plot histogram
fx.hist(mydata['PetalWidthCm'])
# set title and labels
fx.set_title('Histogram for Petal Width')
fx.set_xlabel('Values')
fx.set_ylabel('Count')

fig, gx = plt.subplots()
# plot histogram
gx.hist(mydata['SepalWidthCm'])
# set title and labels
gx.set_title('Histogram for Sepal Width')
gx.set_xlabel('Values')
gx.set_ylabel('Count')

fig, gx = plt.subplots()
# plot histogram
gx.hist(mydata['SepalLengthCm'])
# set title and labels
gx.set_title('Histogram for Sepal Length')
gx.set_xlabel('Values')
gx.set_ylabel('Count')

fig, gx = plt.subplots()
# plot histogram
gx.hist(mydata['PetalLengthCm'])
# set title and labels
gx.set_title('Histogram for Petal Length')
gx.set_xlabel('Values')
gx.set_ylabel('Count')

"""Co-relation Matrix"""

mydata.corr()

corr = mydata.corr()
fig, ax = plt.subplots(figsize=(5,4))
sns.heatmap(corr, annot=True, ax=ax)

sns.scatterplot(x='SepalLengthCm', y='SepalWidthCm', hue='Species', data=mydata)

sns.scatterplot(x='PetalLengthCm', y='PetalWidthCm', hue='Species', data=mydata)

"""Now, we are using label encoder since the target array column is not in the same data type as other attributes"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

mydata['Species'] = le.fit_transform(mydata['Species'])
mydata.head()

"""Let’s convert our arrays to a pandas DataFrame for ease of use. I am setting the column names explicitly."""

x_data= pd.DataFrame(mydata, columns=['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm'])
y_target= pd.DataFrame(mydata, columns=['Species'])
print(x_data)
print(y_target)

"""Clustering"""

from sklearn.cluster import KMeans

"""K Means Clustering

Now we select all four features (sepal length, sepal width, petal length, and petal width) of the dataset in a variable called x so that we can train our model with these features.
"""

## did that in above step its the data with 4 variables
x_data

"""let’s arbitrarily assign the value of k as 5. We will implement k-means clustering using k=5. For this we will instantiate the KMeans class and assign it to the variable kmeans5:"""

kmeans5 = KMeans(n_clusters=5)
y_kmeans5 = kmeans5.fit_predict(x_data)
print(y_kmeans5)

kmeans5.cluster_centers_

"""let’s arbitrarily assign the value of k as 4. We will implement k-means clustering using k=4. For this we will instantiate the KMeans class and assign it to the variable kmeans4:"""

kmeans4 = KMeans(n_clusters=4)
y_kmeans4 = kmeans4.fit_predict(x_data)
print(y_kmeans4)

kmeans4.cluster_centers_

"""To get the optimal value of K lets use elbow method

There’s a method called the Elbow method, which is designed to help find the optimal number of clusters in a dataset. So let’s use this method to calculate the optimum value of k, we’ll plot a graph between the number of clusters and the corresponding error value.
"""

Error =[]
for i in range(1, 11):
    kmeans = KMeans(n_clusters = i).fit(x)
    kmeans.fit(x_data)
    Error.append(kmeans.inertia_)
import matplotlib.pyplot as plt
plt.plot(range(1, 11), Error)
plt.title('Elbow method')
plt.xlabel('No of clusters')
plt.ylabel('Error')
plt.show()

"""The output graph of the Elbow method is shown below. Note that the shape of elbow is approximately formed at k=3,As you can see, the optimal value of k is between 2 and 4, as the elbow-like shape is formed at k=3 in the above graph.
Let’s implement k-means again using k=3
"""

kmeans3 = KMeans(n_clusters=3)
y_kmeans3 = kmeans3.fit_predict(x_data)
print(y_kmeans3)







kmeans3.cluster_centers_

"""Visualizing clustering with optimal K=3"""

X=mydata.iloc[:,[0,1,2,3,4]].values
plt.scatter(X[y_kmeans3 == 0,0], X[y_kmeans3 == 0,1], s=100, c='green', label='Iris-setosa')
plt.scatter(X[y_kmeans3 == 1,0], X[y_kmeans3 == 1,1], s=100, c='blue', label='Iris-versicolor')
plt.scatter(X[y_kmeans3 == 2,0], X[y_kmeans3 == 2,1], s=100, c='red', label='Iris-virginica')


plt.title("cluster of species")
plt.legend()
plt.show()

"""CONCLUSION
1.K-means is simple unsupervised clustering algorithm
2. K means is quick & apt
3. Data set contains 3 classes(setosa, virginica,versi-color) of  50 instances each & has 4 features sepal length, petal length, sepal width &petal width.
4.optimal value of K is 3
5.We have also used many data visualisations

Thankyou
"""